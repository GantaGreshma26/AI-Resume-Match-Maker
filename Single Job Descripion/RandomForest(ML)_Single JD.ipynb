{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuFt/WQqEhZ6SFkVCuD3BQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"source":["!pip install --upgrade PyPDF2 # Upgrade PyPDF2 to the latest version\n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from scipy.sparse import hstack\n","import PyPDF2  # Import PyPDF2 for reading PDF files\n","\n","# Load the training data\n","train_data = pd.read_csv('train.csv')\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","# Concatenate resumes and JDs after handling missing values\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","# Use hstack to concatenate sparse matrices\n","X = hstack([resume_vectors, jd_vectors])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    # Use PdfReader instead of PdfFileReader\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    # Use len(read_pdf.pages) to get the number of pages\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        # Assuming CSV files have a 'Resume' column\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        # Assuming CSV files have a 'JD' column\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","\n","    resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","    feature_vector = hstack([resume_vector, jd_vector])\n","    return rf.predict(feature_vector)[0]\n","\n","# Test the model (replace with actual file paths)\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOTNWi8AJ5BJ","executionInfo":{"status":"ok","timestamp":1718895618253,"user_tz":-330,"elapsed":9011,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"ac736bbc-608f-408d-bc0d-8d2769a4086b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Evaluation of the model:\n","Accuracy: 1.0\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        42\n","           1       1.00      1.00      1.00        42\n","\n","    accuracy                           1.00        84\n","   macro avg       1.00      1.00      1.00        84\n","weighted avg       1.00      1.00      1.00        84\n","\n","Test data result :\n","1\n"]}]},{"cell_type":"code","source":["!pip install --upgrade PyPDF2 # Upgrade PyPDF2 to the latest version\n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from scipy.sparse import hstack\n","import PyPDF2  # Import PyPDF2 for reading PDF files\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load the training data\n","train_data = pd.read_csv('train.csv')\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","# Concatenate resumes and JDs after handling missing values\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","# Use hstack to concatenate sparse matrices\n","X = hstack([resume_vectors, jd_vectors])\n","\n","# Calculate the percentage match for each resume and JD\n","def calculate_percentage_match(resume_text, jd_text):\n","    resume_words = set(word_tokenize(resume_text.lower()))\n","    jd_words = set(word_tokenize(jd_text.lower()))\n","    common_words = resume_words & jd_words\n","    percentage_match = len(common_words) / len(jd_words) * 100\n","    return percentage_match\n","\n","percentage_matches = [calculate_percentage_match(resume, jd) for resume, jd in zip(train_data['Resumes'], train_data['JD'])]\n","\n","# Add the percentage match feature to the feature matrix\n","X = hstack([X, np.array(percentage_matches).reshape(-1, 1)])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    # Use PdfReader instead of PdfFileReader\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    # Use len(read_pdf.pages) to get the number of pages\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        # Assuming CSV files have a 'Resume' column\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        # Assuming CSV files have a 'JD' column\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","\n","    resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","    percentage_match = calculate_percentage_match(resume_text, jd_text)\n","    feature_vector = hstack([resume_vector, jd_vector, np.array([percentage_match]).reshape(1, -1)])\n","    return rf.predict(feature_vector)[0]\n","\n","# Test the model (replace with actual file paths)\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"metadata":{"id":"_M_esq3Cwc16","executionInfo":{"status":"error","timestamp":1718972717013,"user_tz":-330,"elapsed":9246,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"3e023ac9-4f91-454d-a273-90cb9149f81c","colab":{"base_uri":"https://localhost:8080/","height":514}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-b690944a4498>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpercentage_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mpercentage_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcalculate_percentage_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resumes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Add the percentage match feature to the feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-b690944a4498>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpercentage_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mpercentage_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcalculate_percentage_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resumes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Add the percentage match feature to the feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-b690944a4498>\u001b[0m in \u001b[0;36mcalculate_percentage_match\u001b[0;34m(resume_text, jd_text)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mjd_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcommon_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_words\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mjd_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mpercentage_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpercentage_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"cell_type":"code","source":["!pip install --upgrade PyPDF2 # Upgrade PyPDF2 to the latest version\n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.sparse import hstack\n","import PyPDF2  # Import PyPDF2 for reading PDF files\n","\n","# Load the training data\n","train_data = pd.read_csv('train.csv')\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","# Concatenate resumes and JDs after handling missing values\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Calculate the similarity score between resume and JD vectors\n","similarity_scores = cosine_similarity(resume_vectors, jd_vectors)\n","\n","# Calculate the percentage match\n","percentage_match = similarity_scores * 100\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","# Use hstack to concatenate sparse matrices\n","X = hstack([resume_vectors, jd_vectors, percentage_match])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    # Use PdfReader instead of PdfFileReader\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    # Use len(read_pdf.pages) to get the number of pages\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        # Assuming CSV files have a 'Resume' column\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        # Assuming CSV files have a 'JD' column\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","\n","    resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","\n","    # Calculate the similarity score\n","    similarity_score = cosine_similarity(resume_vector, jd_vector)[0][0]\n","\n","    # Calculate the percentage match\n","    percentage_match = similarity_score * 100\n","\n","    feature_vector = hstack([resume_vector, jd_vector, percentage_match])\n","    return rf.predict(feature_vector)[0]\n","\n","# Test the model (replace with actual file paths)\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"YEUwWMUBqH2H","executionInfo":{"status":"error","timestamp":1718971120050,"user_tz":-330,"elapsed":11576,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"c176d821-7292-474a-9a8a-a30bf4cacb55"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-73b1ef4097c1>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Handle missing values (replace NaN with empty strings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UKcUCXUfqrW5"},"execution_count":null,"outputs":[]},{"source":["!pip install --upgrade PyPDF2 # Upgrade PyPDF2 to the latest version\n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.sparse import hstack\n","import PyPDF2  # Import PyPDF2 for reading PDF files\n","\n","# Load the training data\n","# Replace '/path/to/your/train.csv' with the actual path to your file\n","train_data = pd.read_csv('/path/to/your/train.csv')\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","# Concatenate resumes and JDs after handling missing values\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Calculate the similarity score between resume and JD vectors\n","similarity_scores = cosine_similarity(resume_vectors, jd_vectors)\n","\n","# Calculate the percentage match\n","percentage_match = similarity_scores * 100\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","# Use hstack to concatenate sparse matrices\n","X = hstack([resume_vectors, jd_vectors, percentage_match])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    # Use PdfReader instead of PdfFileReader\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    # Use len(read_pdf.pages) to get the number of pages\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        # Assuming CSV files have a 'Resume' column\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        # Assuming CSV files have a 'JD' column\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","\n","    resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","\n","    # Calculate the similarity score\n","    similarity_score = cosine_similarity(resume_vector, jd_vector)[0][0]\n","\n","    # Calculate the percentage match\n","    percentage_match = similarity_score * 100\n","\n","    feature_vector = hstack([resume_vector, jd_vector, percentage_match])\n","    return rf.predict(feature_vector)[0]\n","\n","# Test the model (replace with actual file paths)\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"WOXLzvdGqtHm","executionInfo":{"status":"error","timestamp":1718971202450,"user_tz":-330,"elapsed":6817,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"f37f87dd-7d85-45d6-b0cd-cb949ac16094"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/path/to/your/train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c7d9cf81da48>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Replace '/path/to/your/train.csv' with the actual path to your file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/your/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Handle missing values (replace NaN with empty strings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/train.csv'"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from scipy.sparse import hstack\n","import PyPDF2  # Import PyPDF2 for reading PDF files\n","\n","# Load the training data\n","train_data = pd.read_csv('train.csv')  # Replace with the actual path to your file\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","# Concatenate resumes and JDs after handling missing values\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Calculate the similarity score between resume and JD vectors\n","similarity_scores = cosine_similarity(resume_vectors, jd_vectors)\n","\n","# Calculate the percentage match\n","percentage_match = similarity_scores * 100\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","# Use hstack to concatenate sparse matrices\n","X = hstack([resume_vectors, jd_vectors, percentage_match])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    # Use PdfReader instead of PdfFileReader\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    # Use len(read_pdf.pages) to get the number of pages\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        # Assuming CSV files have a 'Resume' column\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        # Assuming CSV files have a 'JD' column\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","\n","    # Transform the resume and JD texts into TF-IDF vectors\n","    resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","\n","    # Calculate the similarity score\n","    similarity_score = cosine_similarity(resume_vector, jd_vector)[0][0]\n","\n","    # Calculate the percentage match\n","    percentage_match = similarity_score * 100\n","\n","    # Create a feature matrix by concatenating the resume and job description vectors\n","    feature_vector = hstack([resume_vector, jd_vector, [[percentage_match]]])\n","\n","    # Reshape the feature vector to match the expected input shape\n","    feature_vector = feature_vector.reshape(1, -1)\n","    feature_vector = hstack([resume_vector, jd_vector, [[percentage_match]]])\n","    print(feature_vector.shape)  # Check the shape of the feature vector\n","    return rf.predict(feature_vector)[0]\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":639},"id":"pBRTEvwgrTsi","executionInfo":{"status":"error","timestamp":1718972051948,"user_tz":-330,"elapsed":1144,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"33bc9ab3-3f23-4091-c869-ee76439d3ea0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation of the model:\n","Accuracy: 1.0\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        42\n","           1       1.00      1.00      1.00        42\n","\n","    accuracy                           1.00        84\n","   macro avg       1.00      1.00      1.00        84\n","weighted avg       1.00      1.00      1.00        84\n","\n","Test data result :\n","(1, 10001)\n"]},{"output_type":"error","ename":"ValueError","evalue":"X has 10001 features, but RandomForestClassifier is expecting 10084 features as input.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-0be3e5bd8615>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mjd_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jd.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test data result :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-0be3e5bd8615>\u001b[0m in \u001b[0;36mpredict_match\u001b[0;34m(resume_file, jd_file)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresume_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpercentage_match\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check the shape of the feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mresume_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resume.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mjd_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jd.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \"\"\"\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    600\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[1;32m    601\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: X has 10001 features, but RandomForestClassifier is expecting 10084 features as input."]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from scipy.sparse import hstack\n","import PyPDF2\n","\n","# Load the training data\n","train_data = pd.read_csv('train.csv')\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","X = hstack([resume_vectors, jd_vectors])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to calculate the percentage match between resume and JD\n","def calculate_percentage_match(resume_text, jd_text):\n","    resume_words = set(resume_text.split())\n","    jd_words = set(jd_text.split())\n","    common_words = resume_words & jd_words\n","    percentage_match = len(common_words) / len(jd_words) * 100\n","    return percentage_match\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","\n","    resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","    percentage_match = calculate_percentage_match(resume_text, jd_text)\n","    feature_vector = hstack([resume_vector, jd_vector, [[percentage_match]]])\n","    return rf.predict(feature_vector)[0]\n","\n","# Test the model (replace with actual file paths)\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"id":"0JRJmTWRuU3V","executionInfo":{"status":"error","timestamp":1718972147452,"user_tz":-330,"elapsed":523,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"8798feb8-1a49-4741-9853-421c7fce253d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation of the model:\n","Accuracy: 1.0\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        42\n","           1       1.00      1.00      1.00        42\n","\n","    accuracy                           1.00        84\n","   macro avg       1.00      1.00      1.00        84\n","weighted avg       1.00      1.00      1.00        84\n","\n","Test data result :\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-14e1e0f42436>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mjd_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jd.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test data result :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-14e1e0f42436>\u001b[0m in \u001b[0;36mpredict_match\u001b[0;34m(resume_file, jd_file)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mresume_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mjd_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjd_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mpercentage_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_percentage_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresume_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpercentage_match\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-14e1e0f42436>\u001b[0m in \u001b[0;36mcalculate_percentage_match\u001b[0;34m(resume_text, jd_text)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mjd_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mcommon_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_words\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mjd_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mpercentage_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpercentage_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from scipy.sparse import hstack\n","import PyPDF2\n","import numpy as np\n","\n","# Load the training data\n","train_data = pd.read_csv('train.csv')\n","\n","# Handle missing values (replace NaN with empty strings)\n","train_data['Resumes'] = train_data['Resumes'].fillna('')\n","train_data['JD'] = train_data['JD'].fillna('')\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer to the resumes and job descriptions\n","vectorizer.fit(train_data['Resumes'].tolist() + train_data['JD'].tolist())\n","\n","# Transform the resumes and job descriptions into TF-IDF vectors\n","resume_vectors = vectorizer.transform(train_data['Resumes'])\n","jd_vectors = vectorizer.transform(train_data['JD'])\n","\n","# Create a feature matrix by concatenating the resume and job description vectors\n","X = hstack([resume_vectors, jd_vectors])\n","\n","# Create a target vector\n","y = train_data['Result']\n","\n","# Train a Random Forest classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Evaluate the model\n","y_pred = rf.predict(X)\n","print(\"Evaluation of the model:\")\n","print(\"Accuracy:\", accuracy_score(y, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y, y_pred))\n","\n","# Function to read PDF file and extract text\n","def read_pdf(file_path):\n","    pdf_file = open(file_path, 'rb')\n","    read_pdf = PyPDF2.PdfReader(pdf_file)\n","    number_of_pages = len(read_pdf.pages)\n","    text = ''\n","    for page_number in range(number_of_pages):\n","        page = read_pdf.pages[page_number]\n","        page_content = page.extract_text()\n","        text += page_content\n","    return text\n","\n","# Function to calculate the percentage match between resume and JD\n","def calculate_percentage_match(resume_text, jd_text):\n","    resume_words = set(resume_text.split())\n","    jd_words = set(jd_text.split())\n","    common_words = resume_words & jd_words\n","    if len(jd_words) == 0:\n","        percentage_match = 0\n","    else:\n","        percentage_match = len(common_words) / len(jd_words) * 100\n","    return percentage_match\n","\n","# Function to predict the match result\n","def predict_match(resume_file, jd_file):\n","    if resume_file.endswith('.pdf'):\n","        resume_text = read_pdf(resume_file)\n","    else:\n","        resume_text = pd.read_csv(resume_file)['Resume'].iloc[0]\n","\n","    if jd_file.endswith('.pdf'):\n","        jd_text = read_pdf(jd_file)\n","    else:\n","        jd_text = pd.read_csv(jd_file)['JD'].iloc[0]\n","        resume_vector = vectorizer.transform([resume_text])\n","    jd_vector = vectorizer.transform([jd_text])\n","    percentage_match = calculate_percentage_match(resume_text, jd_text)\n","\n","    # Create the feature vector with the correct number of features (10000)\n","    feature_vector = hstack([resume_vector, jd_vector, [[percentage_match]]])\n","\n","    return rf.predict(feature_vector)[0]\n","\n","# Test the model (replace with actual file paths)\n","resume_file = 'resume.pdf'\n","jd_file = 'jd.pdf'\n","print(\"Test data result :\")\n","print(predict_match(resume_file, jd_file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":579},"id":"DrTZrsD9uzwn","executionInfo":{"status":"error","timestamp":1718972557382,"user_tz":-330,"elapsed":1554,"user":{"displayName":"Ganta Greshma","userId":"18051616320727876152"}},"outputId":"1a597c32-3fbe-4dbd-f747-e0b4d7f2245d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation of the model:\n","Accuracy: 1.0\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        42\n","           1       1.00      1.00      1.00        42\n","\n","    accuracy                           1.00        84\n","   macro avg       1.00      1.00      1.00        84\n","weighted avg       1.00      1.00      1.00        84\n","\n","Test data result :\n"]},{"output_type":"error","ename":"UnboundLocalError","evalue":"local variable 'resume_vector' referenced before assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-ac2e60e07a0f>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mjd_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jd.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test data result :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-ac2e60e07a0f>\u001b[0m in \u001b[0;36mpredict_match\u001b[0;34m(resume_file, jd_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Create the feature vector with the correct number of features (10000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresume_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpercentage_match\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'resume_vector' referenced before assignment"]}]}]}